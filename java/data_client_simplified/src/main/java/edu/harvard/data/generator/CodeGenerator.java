package edu.harvard.data.generator;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import com.fasterxml.jackson.core.JsonParseException;
import com.fasterxml.jackson.databind.JsonMappingException;

import edu.harvard.data.DataConfig;
import edu.harvard.data.DataConfigurationException;
import edu.harvard.data.VerificationException;
import edu.harvard.data.identity.IdentifierType;
import edu.harvard.data.schema.DataSchema;
import edu.harvard.data.schema.UnexpectedApiResponseException;
import edu.harvard.data.schema.extension.ExtensionSchema;
import edu.harvard.data.schema.fulltext.FullTextSchema;
import edu.harvard.data.schema.identity.IdentitySchema;

/**
 * Base class for all dataset-specific code generators. It is expected that
 * every data set implementation will provide a subclass of this abstract class
 * in order to generate its SDK, Hadoop identity jobs and other scripts and
 * code.
 * <P>
 * This class uses the Abstract Factory design pattern. It implements the schema
 * transformation and code generation functionality by calling a set of abstract
 * methods defined in the class itself. These abstract methods represent the
 * minimal amount of information that must be provided by an implementing
 * subclass.
 */
public abstract class CodeGenerator {

  private static final Logger log = LogManager.getLogger();

  protected final File codeDir;
  protected final DataConfig config;
  protected final String schemaVersionId;
  private List<SchemaVersion> schemaVersions;
  private IdentitySchema identitySchema;
  private FullTextSchema fullTextSchema;

  /**
   * Initialize the CodeGenerator class with input and output file locations.
   *
   * @param config
   *          the configuration settings for the code generator.
   * @param codeDir
   *          the directory where generated files should be stored. If this
   *          directory does not exist it will be created.
   * @param runId
   *          the unique identifier for the current run, generated by the
   *          initial Lambda bootstrap process.
   */
  public CodeGenerator(final DataConfig config, final File codeDir, final String runId,
      final String schemaVersionId) {
    this.config = config;
    this.codeDir = codeDir;
    this.schemaVersionId = schemaVersionId;
  }

  /**
   * Generate and populate a {@link GenerationSpec} object that contains the
   * parameters and identifiers to be used by generated code. See the
   * documentation for {@link GenerationSpec} for details on the fields that may
   * be added to the spec.
   *
   * @return a populated {@code GenerationSpec} object that will be used to
   *         guide the code generator.
   *
   * @throws IOException
   *           if an error occurs when reading or transforming the data schema.
   * @throws DataConfigurationException
   *           if the subclass encounters a problem when reading some
   *           client-specific configuration file.
   * @throws VerificationException
   *           if an error is detected in the the input schema specifications.
   * @throws UnexpectedApiResponseException
   *           if the subclass encounters an error when interfacing with an
   *           external API.
   */
  // protected abstract GenerationSpec createGenerationSpec() throws
  // IOException,
  // DataConfigurationException, VerificationException,
  // UnexpectedApiResponseException;

  /**
   * Get the common identifier that is used throughout the data set to uniquely
   * identify an individual. See the description of the
   * {@link edu.harvard.data.identity} package for more details on the main
   * identifier.
   *
   * @return an {@code IdentifierType} that indicates the main identifier used
   *         to differentiate users across the data set.
   */
  protected abstract IdentifierType getMainIdentifier();

  /**
   * Get the name of a resource on the class path that contains definitions of
   * any overridden fields in the original schema. This will normally be null,
   * but is required in case a schema supplied by a third party is incorrect.
   *
   * @return a {@code String} containing the name of a classpath resource. A
   *         resource by this name must be available to the classloader that
   *         loaded this class. If there are no field overrides required for the
   *         base schema, this method will return null.
   */
  // protected abstract String getPhaseZeroModificationResource();

  /**
   * Get the name of a resource on the class path that contains the definitions
   * of all user identifiers across the data set. These identifiers will be
   * removed from all models after Phase 1 of processing, and will be used to
   * generate Hadoop jobs to replace identifiers with semantically-meaningless
   * IDs. See {@link IdentitySchema} for details on the format of this resource.
   *
   * @return a {@code String} containing the name of a classpath resource. A
   *         resource by this name must be available to the classloader that
   *         loaded this class.
   */
  // protected abstract String getIdentifierResource();

  /**
   * Get the name of a resource on the class path that contains the definitions
   * of all columns across the data set that contain unbounded text fields.
   * These columns will be stored in S3, since their contents will be truncated
   * in Redshift.
   *
   * @return a {@code String} containing the name of a classpath resource. A
   *         resource by this name must be available to the classloader that
   *         loaded this class.
   */
  // protected abstract String getFullTextResource();

  /**
   * Get the name of a resource on the class path that contains the definitions
   * of any new or modified table that will be produced as a result of running
   * Phase 2. This resource will guide the generation of the Phase 2 models and
   * all later scripts. See {@link ExtensionSchema} for details on the format of
   * this resource.
   *
   * @return a {@code String} containing the name of a classpath resource. A
   *         resource by this name must be available to the classloader that
   *         loaded this class.
   */
  // protected abstract String getSchemaAdditionsResource();

  /**
   * Generate the various Java, Bash and SQL files that are required by the
   * {@link GenerationSpec}. This is the main entry point to the generator;
   * client code should instantiate a subclass of {@code CodeGenerator} and then
   * call its {@code generate} method to kick off the code generation process.
   *
   * @throws IOException
   *           if an error occurs when reading classpath resources or writing to
   *           generated output files.
   * @throws DataConfigurationException
   *           if the subclass encounters a problem when reading some
   *           client-specific configuration file.
   * @throws VerificationException
   *           if an error is detected in the the input schema specifications.
   * @throws UnexpectedApiResponseException
   *           if the subclass encounters an error when interfacing with an
   *           external API.
   */
  protected final void generate() throws IOException, DataConfigurationException,
  VerificationException, UnexpectedApiResponseException {
    codeDir.mkdirs();
    final DataSchema inputSchema = getInputSchema();
    schemaVersions = transformSchema(inputSchema);
    if (getFullTextResource() != null) {
      fullTextSchema = FullTextSchema.read(getFullTextResource());
    } else {
      fullTextSchema = new FullTextSchema();
    }

    // Generate the bindings.
    log.info("Generating Java bindings in " + codeDir);
    new JavaBindingGenerator(this).generate();

    log.info("Generating Java processing steps in " + codeDir);
    new JavaStepsGenerator(this).generate();

    // log.info("Generating Redshift table definitions in " + codeDir);
    // new CreateRedshiftTableGenerator(codeDir, spec, config).generate();
  }

  /**
   * Modify a {@link DataSchema} using a series of transformation resources
   * defined by the abstract methods implemented by subclasses of this type. The
   * result of the transformations are a series of {@code DataSchema} objects
   * that represent the state of the data set at the end of each phase.
   * <P>
   * This method should be called by subclass implementations to take advantage
   * of a standardized transformation process over the set of resources defined
   * by the abstract methods of this class.
   *
   * @param base
   *          the original {@code DataSchema} before any transformations are
   *          applied. This can be thought of as the Phase 0 schema,
   *          representing the set of tables either directly obtained from the
   *          data source or minimally transformed in order to fit into the
   *          table structure required by the rest of the pipeline.
   *
   * @return a {@link List} of {@code DataSchema} objects representing the
   *         schema at the end of each phase in the pipeline. Thus to get the
   *         schema at the end of Phase 2, a client would use
   *         {@code transformSchema(base).get(2);}
   *
   * @throws VerificationException
   *           if an error is detected in the the input schema specifications.
   * @throws IOException
   *           if an error occurs when reading classpath resources.
   */
  // public final List<DataSchema> transformSchema(final DataSchema base)
  // throws VerificationException, IOException {
  // // We will transform the schema using the additions specified in json
  // // resources.
  // final ExtensionSchema additions = ExtensionSchema
  // .readExtensionSchema(getSchemaAdditionsResource());
  //
  // final SchemaTransformer transformer = new SchemaTransformer();
  //
  // // Update the schema with any overrides required due to errors in a third
  // // party schema document
  // final String overrides = getPhaseZeroModificationResource();
  // final DataSchema inputSchema;
  // if (overrides == null) {
  // inputSchema = base;
  // } else {
  // final ExtensionSchema overrideExtensions =
  // ExtensionSchema.readExtensionSchema(overrides);
  // inputSchema = transformer.transform(base, overrideExtensions, true);
  // for (final DataSchemaTable table : inputSchema.getTables().values()) {
  // table.setNewlyGenerated(false);
  // for (final DataSchemaColumn column : table.getColumns()) {
  // column.setNewlyGenerated(false);
  // }
  // }
  // }
  //
  // // Transform the schema to remove identifers
  // final IdentitySchema identities =
  // IdentitySchema.read(getIdentifierResource());
  // final IdentitySchemaTransformer idTrans = new
  // IdentitySchemaTransformer(inputSchema, identities,
  // getMainIdentifier());
  // final DataSchema deidentified = idTrans.transform();
  //
  // final DataSchema outputSchema = transformer.transform(deidentified,
  // additions, false);
  //
  // final List<DataSchema> schemas = new ArrayList<DataSchema>();
  // schemas.add(inputSchema);
  // schemas.add(deidentified);
  // schemas.add(outputSchema);
  // return schemas;
  // }

  protected abstract String getInputSchemaOverrideResource();

  protected abstract String getIdentifierResource();

  protected abstract String getFullTextResource();

  protected abstract List<String> getCustomTransformationResources();

  protected abstract String getJavaProjectName();

  protected abstract String getJavaTableEnumName();

  protected abstract String getJavaPackageBase();

  public abstract String getGeneratedCodeManagerClassName();

  protected abstract DataSchema getInputSchema() throws JsonParseException, JsonMappingException,
  IOException, DataConfigurationException, UnexpectedApiResponseException;

  public final List<SchemaVersion> transformSchema(final DataSchema inputSchema)
      throws VerificationException, IOException {
    final SchemaTransformer transformer = new SchemaTransformer();
    final String bindingPackageBase = getJavaPackageBase();
    final List<SchemaVersion> schemaVersions = new ArrayList<SchemaVersion>();
    // First we need to figure out what the input schema looks like. In a
    // perfect world this would be correctly specified by some external resource
    // (api, schema document or so on), but we need to account for the case
    // where there is an error in third party documentation.
    final String overrideResource = getInputSchemaOverrideResource();
    final DataSchema phase0Schema;
    if (overrideResource == null) {
      // The input schema doesn't need to be updated. Set it as the correct
      // version.
      phase0Schema = inputSchema;
    } else {
      // There is some change to the input schema. Parse the overrides
      // transformation spec, and modify the input schema as necessary.
      final ExtensionSchema overrides = ExtensionSchema.readExtensionSchema(overrideResource);
      phase0Schema = transformer.transform(inputSchema, overrides, true);
      // XXX: Set the updated schema tables and columns to newly generated?
    }
    schemaVersions.add(new SchemaVersion(phase0Schema, bindingPackageBase, 0));

    // Next we need to do a specific transformation step to remove identities
    identitySchema = IdentitySchema.read(getIdentifierResource());
    final IdentitySchemaTransformer idTrans = new IdentitySchemaTransformer(phase0Schema,
        identitySchema, getMainIdentifier());
    final DataSchema deidentified = idTrans.transform();
    schemaVersions.add(new SchemaVersion(deidentified, bindingPackageBase, 1));

    // Finally, we transform the schema according to any custom transformations
    // that the data source calls for.
    final List<String> extensionResources = getCustomTransformationResources();
    DataSchema previousSchema = deidentified;
    for (int i = 0; i < extensionResources.size(); i++) {
      final String resource = extensionResources.get(i);
      final ExtensionSchema extension = ExtensionSchema.readExtensionSchema(resource);
      final DataSchema transformed = transformer.transform(previousSchema, extension, true);
      final SchemaVersion schemaVersion = new SchemaVersion(transformed, bindingPackageBase, i + 2);
      schemaVersions.add(schemaVersion);
      previousSchema = transformed;
    }
    return schemaVersions;
  }

  public File getOutputBase() {
    return codeDir;
  }

  public List<SchemaVersion> getSchemaVersions() {
    return schemaVersions;
  }

  public String getJavaIdentityStepPackage() {
    return getJavaPackageBase() + ".identity";
  }

  public String getJavaFullTextStepPackage() {
    return getJavaPackageBase() + ".full_text";
  }

  public String getSchemaVersionId() {
    return schemaVersionId;
  }

  public IdentitySchema getIdentitySchema() {
    return identitySchema;
  }

  public FullTextSchema getFullTextSchema() {
    return fullTextSchema;
  }

}
